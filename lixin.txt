1、物理机
禁用 selinux 和 firewalld
修改配置文件 /etc/selinux/config
SELINUX=disabled

禁用 firewalld
systemctl stop firewalld
systemctl mask firewalld

重启系统
#-------------------------------------------#
网络 yum 源
1 安装 FTP 服务
yum install -y vsftpd
/etc/vsftpd/vsftpd.conf
listen=YES
listen_ipv6=NO

systemctl start vsftpd 

验证 ftp 服务
lftp 命令
lftp ip.xx.xx.xx -u username 默认连接 ftp 服务
lftp sftp://ip.xx.xx.xx -u username 连接 ssh 服务

2 mount iso
mkdir -p /var/ftp/CentOS7
mount -t iso9660 -o ro,loop /var/iso/CentOS7-

1708.iso  /var/ftp/CentOS7

3 创建自定义 yum 源
mkdir -p /var/ftp/public
cp xx.rpm /var/ftp/public
cd /var/ftp/public
createrepo  .             创建新的
createrepo  --update  .   更新

客户端配置
/etc/yum.repos.d/ooxx.repo
[local_centos]
name=CentOS 7 Source
baseurl=ftp://ip.xx.xx.xx/CentOS7
enabled=1
gpgcheck=0

[local_soft]
name=local soft
baseurl=ftp://ip.xx.xx.xx/public
enabled=1
gpgcheck=0

#-------------物理机-------------#
转发 dns 安装配置
yum install -y bind bind-chroot
配置文件 /etc/named.conf
listen-on port 53 { 192.168.1.10; };
//listen-on-v6 port 53 { ::1; };
allow-query     { any; };
forwarders { 202.106.196.115; };
dnssec-enable no;
dnssec-validation no;

时间服务器，server、 client
yum install -y chrony 
server:配置 /etc/chrony.conf
server ntp1.aliyun.com iburst
bindacqaddress 0.0.0.0
allow 0/0

client:配置 /etc/chrony.conf
server server.ip.xx.xx iburst

检查状态
chronyc sources -v
* 同步成功，+ 备胎 , ? 同步失败

shell 命令 exec 重定向

#------------快速创建虚拟机---------------#
virsh undefine node
创建前端盘
cd  /var/lib/libvirt/images/
qemu-img create -f qcow2 -b node.qcow2 node1.img 30G
创建 xml 配置文件
sed 's/demo/node1/' demo.xml >/etc/libvirt/qemu/node1.xml
定义虚拟机
virsh define /etc/libvirt/qemu/node1.xml
virsh start node1

添加网卡
    <interface type='bridge'>
      <source bridge='private1'/>
      <model type='virtio'/>
    </interface>

添加硬盘
cd /var/lib/libvirt/images/
qemu-img create -f qcow2 disk.img 20G

    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/disk.img'/>
      <target dev='vdb' bus='virtio'/>
    </disk>

#-----------------------------------------#
创建 2 个虚拟机
要求：
虚拟机1 
  8G 内存
  2块网卡
     第一块网卡连接虚拟交换机 vbr
     第二块网卡连接虚拟交换机 private1
     设置静态 IP，默认网关 192.168.1.254
/etc/sysconfig/network-scripts/ifcfg-ethX
BOOTPROTO="static"
IPADDR="192.168.1.xx"
PREFIX="24"

默认网关:
GATEWAY="192.168.1.254"

  2块硬盘
  其中 / 的 vda1 要求 50G
  vdb  20G

虚拟机2
  6G 内存
  2块网卡
     第一块网卡连接虚拟交换机 vbr
     第二块网卡连接虚拟交换机 private1
     设置静态 IP，默认网关 192.168.1.254
     要求与第一台虚拟主机的两个 IP 都能 ping 通
  硬盘
     / 的 vda1 要求 50G

#---------------openstack 安装---------------------#
yum 源的安装配置
在服务器上 mount 3 个 iso 文件
mount -t iso9660 -o ro,loop RHEL7OSP-10.iso  /var/ftp/openstack
mount -t iso9660 -o ro,loop RHEL7-extras.iso  /var/ftp/openstack-ext

[root@openstack yum.repos.d]# cat openstack.repo 
[openstack_extras]
name=openstack extras
baseurl="ftp://192.168.1.254/openstack-ext"
enabled=1
gpgcheck=0

[rhel-7-server-openstack-10-devtools-rpms]
name=rhel-7-server-openstack-10-devtools-rpms
baseurl=ftp://192.168.1.254/openstack/rhel-7-server-openstack-10-devtools-rpms
enabled=1
gpgcheck=0

[rhel-7-server-openstack-10-optools-rpms]
name=rhel-7-server-openstack-10-optools-rpms
baseurl=ftp://192.168.1.254/openstack/rhel-7-server-openstack-10-optools-rpms
enabled=1
gpgcheck=0

[rhel-7-server-openstack-10-rpms]
name=rhel-7-server-openstack-10-rpms
baseurl=ftp://192.168.1.254/openstack/rhel-7-server-openstack-10-rpms
enabled=1
gpgcheck=0

[rhel-7-server-openstack-10-tools-rpms]
name=rhel-7-server-openstack-10-tools-rpms
baseurl=ftp://192.168.1.254/openstack/rhel-7-server-openstack-10-tools-rpms
enabled=1
gpgcheck=0

[rhel-7-server-rhceph-2-mon-rpms]
name=rhel-7-server-rhceph-2-mon-rpms
baseurl=ftp://192.168.1.254/openstack/rhel-7-server-rhceph-2-mon-rpms
enabled=1
gpgcheck=0

[rhel-7-server-rhceph-2-osd-rpms]
name=rhel-7-server-rhceph-2-osd-rpms
baseurl=ftp://192.168.1.254/openstack/rhel-7-server-rhceph-2-osd-rpms
enabled=1
gpgcheck=0

[rhel-7-server-rhceph-2-tools-rpms]
name=rhel-7-server-rhceph-2-tools-rpms
baseurl=ftp://192.168.1.254/openstack/rhel-7-server-rhceph-2-tools-rpms
enabled=1
gpgcheck=0

[rhel-7-server-rhscon-2-agent-rpms]
name=rhel-7-server-rhscon-2-agent-rpms
baseurl=ftp://192.168.1.254/openstack/rhel-7-server-rhscon-2-agent-rpms
enabled=1
gpgcheck=0

[rhel-7-server-rhscon-2-installer-rpms]
name=rhel-7-server-rhscon-2-installer-rpms
baseurl=ftp://192.168.1.254/openstack/rhel-7-server-rhscon-2-installer-rpms
enabled=1
gpgcheck=0

[rhel-7-server-rhscon-2-main-rpms]
name=rhel-7-server-rhscon-2-main-rpms
baseurl=ftp://192.168.1.254/openstack/rhel-7-server-rhscon-2-main-rpms
enabled=1
gpgcheck=0

配置完成以后 yum repolist 能看到 12 个源及 10,731 个包

#--------------openstack-----------------#
物理机 == 真机
虚拟机

物理机：
    安装 bind（dns）服务
    安装 chrond（ntp）服务

虚拟机：
    /etc/resolv.conf 配置 物理机的IP
    nameserver 192.168.1.254
    /etc/chrony.conf 配置指向物理机的IP
    server 192.168.1.254 iburst

    验证：
    ping  www.baidu.com
    chronyc sources -v

虚拟机 2 台，openstack(8G) ,nova(6G)
openstack: 
    pvcreate /dev/vdb
    vgcreate cinder-volumes /dev/vdb

openstack, nova:
    yum install -y qemu-kvm \
                   libvirt-client \
                   libvirt-daemon \
                   libvirt-daemon-driver-qemu \
                   python-setuptools

openstack:
    yum install -y openstack-packstack
    packstack --gen-answer-file answer.ini
    编辑 answer.ini
配置默认密码
11:  CONFIG_DEFAULT_PASSWORD=Taren1
设置禁用 swift (对象存储) 模块
42:  CONFIG_SWIFT_INSTALL=n
NTP 服务器地址
75:  CONFIG_NTP_SERVERS=192.168.1.254
计算节点IP地址
98:  CONFIG_COMPUTE_HOSTS=192.168.1.19
需要配置vxlan网络的 IP 地址
102: CONFIG_NETWORK_HOSTS=192.168.1.19
禁用自动创建 cinder-volumns 卷组
554: CONFIG_CINDER_VOLUMES_CREATE=n
设置网络支持协议
840: CONFIG_NEUTRON_ML2_TYPE_DRIVERS=flat,vxlan
设置组播地址
876: CONFIG_NEUTRON_ML2_VXLAN_GROUP=239.1.1.5
设置虚拟交换机
910: CONFIG_NEUTRON_OVS_BRIDGE_MAPPINGS=physnet1:br-ex
设置虚拟交换机所连接的物理网卡
921: CONFIG_NEUTRON_OVS_BRIDGE_IFACES=br-ex:eth0
设置隧道网络使用的网卡
936: CONFIG_NEUTRON_OVS_TUNNEL_IF=eth1
禁用测试的DEMO
1179:CONFIG_PROVISION_DEMO=n

    保持配置文件，安装 openstack
    packstack --answer-file=answer.ini

    horizon 软件配置 bug
    修改 /etc/httpd/conf.d/15-horizon_vhost.conf
    添加 WSGIApplicationGroup %{GLOBAL}
    重新载入配置 apachectl graceful

#-----------------------------------------------#
关于 /root/keystone_admin 文件的注意事项：
  1、该文件只是一个文本，里面记录了 admin 的用户名和密码，与认证无关
  2、不能通过修改该文件达到修改 admin 密码的目的
  3、修改 admin 密码请使用 web 页面登录后，在web页面中修改
     修改后，keystone_admin 中的密码通用要修改成新的密码才可以继续使用
  4、该文件可以作为命令行的登录文件使用，使用方式 source ~/keystone_admin
  5、如果你安装完成 openstack 后，没有修改过 admin 的密码
     keystone_admin 文件不小心被更改了
     我们可以通过查找应答文件 answer.ini 中的变量 CONFIG_KEYSTONE_ADMIN_PW 来找到默认安装密码

#---------------------nova安装------------------#
1 检查nova主机能否与 openstack 互相ping 通(两块网卡都要测试)
2 检查 selinux 是否禁用 (sestatus)
3 检查 firewalld 是否卸载 (rpm -qa|grep firewalld)
4 检查 NetworkManager 是否卸载 (rpm -qa|grep NetworkManager)
5 检查 yum 源 (yum repolist   10731包)
6 检查 系统 yum 源是否导入公钥 (gpgcheck=1)
7 检查 时间服务器 (chronyc sources -v)
8 检查 openstack 和 本机的主机名 是否能 ping 通

openstack:
  编辑 answer.ini
98:  CONFIG_COMPUTE_HOSTS=192.168.1.19,192.168.1.18
102: CONFIG_NETWORK_HOSTS=192.168.1.19,192.168.1.18
  packstack --answer-file=answer.ini

常见云服务商评测报告
https://wenku.baidu.com/view/c9bab20c49d7c1c708a1284ac850ad02df800719.html

监控宝
https://baike.baidu.com/item/%E7%9B%91%E6%8E%A7%E5%AE%9D/3661540?fr=aladdin





1 docker 安装
#----------------------物理机--------------------------#
创建一台 4G 内存的虚拟机
创建虚拟机硬盘
qemu-img create -b node.qcow2 -f qcow2 node1.img 20G
创建虚拟机配置文件
sed 's/demo/node1/' demo.xml >/etc/libvirt/qemu/node1.xml
定义虚拟机
virsh define /etc/libvirt/qemu/node1.xml
编辑虚拟机修改内存
virsh edit node1
启动虚拟机
virsh start node1

创建自定义 yum 源
拷贝 docker-engine*  /var/ftp/public
cd /var/ftp/public
createrepo  .

#--------------------虚拟机--------------------#
修改主机名和静态 ip 地址
echo docker01 >/etc/hostname
配置静态 ip 地址 /etc/sysconfig/network-scripts/ifcfg-eth0 
BOOTPROTO="static"
IPADDR="192.168.1.10"
NETMASK="255.255.255.0"
GATEWAY="192.168.1.254"

配置 /etc/yum.repos.d/docker.repo
[local_docker]
name=CentOS docker
baseurl="ftp://192.168.1.254/public"
enabled=1
gpgcheck=0

安装 docker
yum install docker-engine
设置开机自启动
systemctl enable docker
重启虚拟机
reboot

docker 常用命令
查看版本
docker version
查看镜像
docker images
搜索镜像
docker search 关键字
下载镜像
docker pull 镜像的名字
查看命令帮助
docker help 命令
导出镜像
docker save busybox:latest >busybox.tar
导入镜像
docker load <busybox.tar

启动容器
docker run -it  镜像的名字:标签  镜像里面的命令
查看容器
docker ps -a [查看所有容器]

删除镜像
docker rmi 镜像名称:标签
创建镜像别名
docker tag 源镜像名称:标签 新名称:新标签

容器命令介绍
docker run 
-i 交互式
-t 终端
-d 后台进程

启动一个交互式的终端 /bin/bash /usr/bin/python
docker run -it centos /bin/bash
docker run -it centos /usr/bin/python

启动一个服务(非交互) nginx 
docker run -d nginx

启动一个交互式的终端，放在后台运行 /bin/bash &
docker run -itd centos

查看容器  docker ps
-a 所有容器
-q 只显示id
停止所有正在运行中的容器
docker stop $(docker ps -q)
删除所有容器
docker rm $(docker ps -aq)
查看容器内进程列表
docker top 容器id
查看容器详细信息，查看容器 ip
docker inspect 容器id
docker inspect -f "{{.NetworkSettings.IPAddress}}"  容器id

进入一个已经运行的容器
docker exec -it 容器id /bin/bash

连接一个已经运行容器的 pid为1 的进程
退出后容器会结束，如果不想结束需要把容器放入后台(ctrl + pq)
docker attach 容器id
#------------------------------------------#
        redhat              debina
#------------------------------------------#
      RHEL,CentOS           debina
        fedora              ubuntu
安装      yum               apt-get
管理      rpm                dpkg
#------------------------------------------#

编排镜像
1、创建容器
docker run -it centos
2、修改配置、安装软件
3、创建镜像
docker ps -a
docker commit 容器ID  镜像名称:标签

Dockerfile 01
FROM centos:latest
RUN  rm -f /etc/yum.repos.d/*
ADD  local.repo /etc/yum.repos.d/local.repo
RUN  yum install -y net-tools psmisc lftp iproute vim

Dockerfile 02
FROM myos:latest
CMD ["/usr/bin/pythono"]

Dockerfile 03
FROM myos
RUN  yum install -y httpd
ADD  httpd.conf /etc/httpd/conf/httpd.conf
WORKDIR /var/www/html
RUN  echo "hello nsd1803" >index.html
EXPOSE 80
MAINTAINER lixin
ENV  EnvironmentFile=/etc/sysconfig/httpd
CMD  ["/usr/sbin/httpd", "-DFOREGROUND"]

创建私有仓库
配置文件 /etc/docker/daemon.json
{
  "insecure-registries" : ["192.168.1.10:5000"]
}

完成配置以后重启 docker 服务
systemctl restart docker

启动私有仓库服务
docker run -d -p 5000:5000 registry

打标记
docker tag busybox:latest 192.168.1.10:5000/busybox:latest
docker tag myos:latest 192.168.1.10:5000/myos:latest
docker tag myos:python 192.168.1.10:5000/myos:python
docker tag myos:httpd  192.168.1.10:5000/myos:httpd

上传镜像
docker push 192.168.1.10:5000/busybox:latest
docker push 192.168.1.10:5000/myos:latest
docker push 192.168.1.10:5000/myos:python
docker push 192.168.1.10:5000/myos:httpd

客户机使用私有镜像源
配置 daemon.json
重启服务 systemctl restart docker
启动容器
docker run -it 192.168.1.10:5000/busybox
docker run -it 192.168.1.10:5000/myos
docker run -d 192.168.1.10:5000/myos:httpd

查看私有仓库有什么样的镜像
curl http://192.168.1.10:5000/v2/_catalog
查看私有仓库的镜像有什么样的标签
curl http://192.168.1.10:5000/v2/myos/tags/list

存储卷映射
docker run -itd -v 物理机文件夹:容器内文件夹 镜像:标签

创建虚拟交换机
docker network create --driver=bridge --subnet 192.168.100.0/24 docker1
创建一个新的容器，使用新的交换机
docker run -it --network=docker1 myos

创建容器，使用宿主机的端口 -p 宿主机端口:容器端口
docker run -d -p 80:80 -v /var/webroot:/var/www/html 192.168.1.10:5000/myos:httpd




ansible 01
  1 ansible  192.168.1.10   管理机器
  2 web1     192.168.1.11   托管机器
  3 web2     192.168.1.12   托管机器
  4 db1      192.168.1.21   托管机器
  5 db2      192.168.1.22   托管机器
  6 cache    192.168.1.33   托管机器

在 ansible 管理机器上,配置源
yum install -y ansible

安装完成以后执行，没有报错，正确显示版本即可
ansible --version

ansible 的配置文件是 ansible.cfg 
ansible.cfg 的查找顺序是 
1  ANSIBLE_CONFIG 变量定义的配置文件
2 当前目录下的 ./ansible.cfg 文件
3 前用户家目录下 ~/ansible.cfg 文件
4 /etc/ansible/ansible.cfg 文件

ansible.cfg 中 inventony 指定主机分组文件的路径和地址，默认分组文件 hosts
hosts 的配置
[web]
web[1:2]

[db]
db1
db2

[app:children]  # 指定子组
web
db

[app:vars]
ansible_ssh_user="root"
ansible_ssh_pass="123456"

[other]
cache		ansible_ssh_user="root" ansible_ssh_pass="123456"

动态主机
ansible 的inventony 文件可以是静态也可以是脚本(输出格式 json)
修改 ansible.cfg 
inventory      = urscript

一个shell 脚本样例
#!/bin/bash
echo '
{
    "web"   : ["web1", "web2"],
    "db"    : ["db1", "db2"],
    "other" : ["cache"]
}'

ansible  命令基础
ansible  主机分组  -m 模块  -a '命令和参数'

创建密钥对 id_rsa 是私钥，  id_rsa.pub 是公钥
ssh-keygen -t rsa -b 2048 -N ''

给所有主机部署密钥
ansible all -m authorized_key -a "user=root exclusive=true manage_dir=true key='$(< /root/.ssh/id_rsa.pub)'" -k

模块 
ansible-doc  查看帮助，必须掌握
ansible-doc -l  列出所有模块
ansible-doc  模块名   查看该模块的帮助信息

ping
没有参数，检测主机的连通性，与 ping 无关，主要检测 ssh 是否可以连接

command | shell | raw 
command 是默认模块，没有启用 shell ，所有shell 相关特性命令无法使用，例如 < > | &
raw    模块，没有 chdir create remove 等参数，能执行大部分操作
shell  模块，启动 /bin/sh 运行命令，可以执行所有操作

测试
ansible cache -m command -a 'chdir=/tmp touch f1'   创建成功
ansible cache -m shell -a 'chdir=/tmp touch f2'     创建成功
ansible cache -m raw -a 'chdir=/tmp touch f3'       文件可以创建，但无法切换目录，文件在用户家目录下生成

复杂操作怎么办，使用脚本来解决
#!/bin/bash
id  zhang3
if [ $? != 0 ];then
    useradd li4
    echo 123456 |passwd --stdin li4
fi
ansible all -m scriopt -a "urscriopt.sh"

copy lineinfile replace 模块
copy 把文件发布到远程其他主机上面
lineinfile  修改一个文件的一行，以行为基础，整行修改
replace    修改文件的某一部分，以正则表达式匹配为基础修改

利用 copy 模块修改所有机器的 /etc/resolv.conf 为 
nameserver 8.8.8.8

利用 lineinfile 修改 /etc/sysconfig/network-scriopts/ifcfg-eth0 
ONBOOT=yes|no
ansible cache -m lineinfile -a 'path=/etc/sysconfig/network-scripts/ifcfg-eth0 regexp="^ONBOOT" line="ONBOOT=\"no\""

利用 replace 修改 /etc/sysconfig/network-scriopts/ifcfg-eth0 
ONBOOT=no|yes
ansible cache -m replace -a 'path=/etc/sysconfig/network-scripts/ifcfg-eth0 regexp="^(ONBOOT=).*" replace="\1\"yes\""'

yum 模块  installed 安装， removed 删除
ansible other -m yum -a 'name="lrzsz" state=removed' 
ansible other -m yum -a 'name="lftp"  state=removed'
ansible other -m yum -a 'name="lrzsz,lftp" state=installed'

service 模块  name 指定服务名称，enabled= yes|no 设置开机启动， state=stopped|started 启动关闭服务

设置 chronyd 服务开启启动，并启动服务
ansible other -m service -a 'name="chronyd" enabled="yes"  state="started"'

setup 模块，查看信息  filter 过滤指定的关键字

playbook httpd.yml
---
- hosts: web
  remote_user: root
  tasks:
    - name: install the latest version of Apache
      yum:
        name: httpd
        state: installed
    - lineinfile:
        path: /etc/httpd/conf/httpd.conf
        regexp: '^Listen '
        insertafter: '^#Listen '
        line: 'Listen 8080'
    - lineinfile:
        path: /etc/httpd/conf/httpd.conf
        regexp: '^#ServerName'
        line: 'ServerName localhost'
    - copy:
        src: /root/index.html
        dest: /var/www/html/index.html
        owner: apache
        group: apache
        mode: 0644
    - service:
        name: httpd
        state: started
        enabled: yes

使用变量和过滤器添加用户
---
- hosts: cache
  remote_user: root
  vars:
    un: nb
  tasks:
    - user:
        name: "{{un}}"
        group: users
        password: "{{'123456'|password_hash('sha512')}}"
    - name: ooxx
      shell: chage -d 0 "{{un}}"

添加用户，忽略错误
---
- hosts: db
  remote_user: root
  vars:
    un: zhang3
  tasks:
    - shell: adduser "{{un}}"
      ignore_errors: True
    - shell: echo 123456|passwd --stdin "{{un}}"
    - name: ooxx
      shell: chage -d 0 "{{un}}"


handlers 触发
---
- hosts: web
  remote_user: root
  tasks:
    - name: install the latest version of Apache
      yum:
        name: httpd
        state: installed
    - lineinfile:
        path: /etc/httpd/conf/httpd.conf
        regexp: '^Listen '
        insertafter: '^#Listen '
        line: 'Listen 8080'
      notify:
        - restart_httpd
    - lineinfile:
        path: /etc/httpd/conf/httpd.conf
        regexp: '^#ServerName'
        line: 'ServerName localhost'
      notify:
        - restart_httpd
    - copy:
        src: /root/index.html
        dest: /var/www/html/index.html
        owner: apache
        group: apache
        mode: 0644
    - copy:
        src: /root/httpd.conf
        dest: /etc/httpd/conf/httpd.conf
        owner: root
        group: root
        mode: 0644
      tags: config_httpd
      notify:
        - restart_httpd
  handlers:
    - name: restart_httpd
      service:
        name: httpd
        state: restarted
        enabled: yes

把负载高的web服务器停止
---
- hosts: web
  remote_user: root
  tasks:
    - shell: uptime |awk '{printf("%.2f",$(NF-2))}'
      register: result
    - service: name=httpd state=stopped
      when: result.stdout|float > 0.7
追加 debug 调试信息
    - name: Show debug info
      debug: var=ooxx

测试命令 awk 'BEGIN{while(1){}}'

循环添加多用户
---
- hosts: web2
  remote_user: root
  tasks:
    - user:
        name: "{{item.name}}"
        group: "{{item.group}}"
        password: "{{'123456'|password_hash('sha512')}}"
      with_items:
        - {name: "nb", group: "users"}
        - {name: "dd", group: "mail" }
        - {name: "jj", group: "wheel"}
        - {name: "lx", group: "root" }

嵌套循环
---
- hosts: cache
  remote_user: root
  vars:
    un: [a, b, c]
    id: [1, 2, 3]
  tasks:
    - name: add users
      shell: echo {{item}}
      with_nested:
        - "{{un}}"
        - "{{id}}"

标签设置 tags: config_httpd

检测语法
ansible-playbook  --syntax-check  playbook.yaml

测试运行
ansible-playbook -C  playbook.yaml

显示受到影响到主机 --list-hosts
显示工作的 task --list-tasks
显示将要运行的 tag --list-tags




重定向  <   >   >>  <<
stdin	0	标准输入(读)
stdout	1	标准输出(写)
stderr	2	标准错误(写)

exec 修改文件描述符号

elasticsearch 单机安装
1 修改 /etc/hosts
2 安装 yum install -y java-1.8.0-openjdk
3 安装 yum install -y elasticsearch
修改配置文件 /etc/elasticsearch/elasticsearch.yml
network.host: 0.0.0.0
启动服务 systemctl start elasticsearch

浏览器访问验证 http://192.168.1.11:9200/

elasticsearch 集群安装
一共安装 5 台 ES 数据库节点
配置所有主机的 /etc/hosts
192.168.1.11    es1
192.168.1.12    es2
192.168.1.13    es3
192.168.1.14    es4
192.168.1.15    es5

在所有节点安装 
yum install -y java-1.8.0-openjdk elasticsearch

修改配置文件
cluster.name: nsd1803
node.name: 本机主机名称
network.host: 0.0.0.0
discovery.zen.ping.unicast.hosts: ["es1", "es2", "es3"]

验证集群
http://192.168.1.11:9200/_cluster/health?pretty

插件安装
/usr/share/elasticsearch/bin/plugin install ftp://192.168.1.254/public/bigdesk-master.zip
/usr/share/elasticsearch/bin/plugin install ftp://192.168.1.254/public/elasticsearch-head-master.zip
/usr/share/elasticsearch/bin/plugin install ftp://192.168.1.254/public/elasticsearch-kopf-master.zip

访问插件 head
http://192.168.1.11:9200/_plugin/head
访问插件 kopf
http://192.168.1.11:9200/_plugin/kopf
访问插件 bigdesk
http://192.168.1.11:9200/_plugin/bigdesk

创建索引
curl -XPUT 'http://192.168.1.13:9200/tedu/' -d \
'{
    "settings":{
        "index":{
            "number_of_shards": 5,
            "number_of_replicas": 1
        }
    }
}'

增加数据 PUT
curl -XPUT "http://192.168.1.11:9200/nsd1803/teacher/2" -d \
'{
  "title": "阶段2",
  "name": {"first":"老逗比", "last":"丁丁"},
  "age":52
}'

更改数据 POST
curl -XPOST "http://192.168.1.15:9200/nsd1803/teacher/3/_update" -d \
'{
  "doc": { "age":18 }
}'

查询与删除数据
curl -XGET    "http://192.168.1.14:9200/nsd1803/teacher/1?pretty"
curl -XDELETE "http://192.168.1.14:9200/nsd1803/teacher/1?pretty"

kibana 配置 /opt/kibana/config/kibana.yml
server.port: 5601
server.host: "0.0.0.0"
elasticsearch.url: "http://es1:9200"
kibana.index: ".kibana"
kibana.defaultAppId: "discover"
elasticsearch.pingTimeout: 1500
elasticsearch.requestTimeout: 30000
elasticsearch.startupTimeout: 5000

数据批量导入
有 index, type, id 的导入
curl -XPOST http://192.168.1.13:9200/_bulk --data-binary @shakespeare.json

无 index, type 有 id 的导入
curl -XPOST http://192.168.1.13:9200/oo/xx/_bulk --data-binary @accounts.json

有 多个index,type 无 id 的导入
curl -XPOST http://192.168.1.13:9200/_bulk --data-binary @logs.jsonl

数据批量查询
curl -XGET http://192.168.1.12:9200/_mget?pretty -d '
{ 
  docs:[
    {"_index": "oo",
     "_type": "xx",
     "_id": 99
    },
    {"_index": "shakespeare",
     "_type": "act",
     "_id": 80730
    },
    {"_index": "logstash-2015.05.20",
     "_type": "log",
     "_id": "AWTo2xXm16RGslV6jxJR"
    }
  ]
}'

logstash 配置文件 logstash.conf 
input{
  stdin{ codec => "json" }
}

filter{

}

output{
  stdout{ codec => "rubydebug" }
}

源码地址 https://github.com/logstash-plugins
文档地址 https://www.elastic.co/guide/en/logstash/current/index.html

file 模块
input {
  ... ...
  file {
    path => ["/tmp/a.log", "/tmp/b.log"]
    sincedb_path => "/var/lib/logstash/sincedb.log"
    start_position => "beginning"
    type => "filelog"
  }
}

tcp & udp 模块
input {
  ... ...
  tcp {
    mode => "server"
    host => "0.0.0.0"
    port => 8888
    type => "tcplog"
  }
  udp {
    port => 9999
    type => "udplog"
  }
}

测试命令
echo "test udp log" >/dev/udp/192.168.1.20/9999
echo "test tcp log" >/dev/tcp/192.168.1.20/8888

syslog 模块

input {
  ... ...
  syslog {
    host => "0.0.0.0"
    port => 514
    type => "syslog"
  }
}

客户机配置 @@(tcp)  @(udp)
local0.info                     @@192.168.1.20:514
命令
logger -p local0.info -t "testlog" "hello world"

正则表达式分组匹配 (?<name>reg)

正则宏路径
/opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-2.0.5/patterns

filter{
  grok{
    match => ["message", "%{COMBINEDAPACHELOG}"]
  }
}

output 输出到 Elasticsearch
增加类型判断，细化区分日志
output{
  if [type] == "apachelog"{
  elasticsearch {
      hosts => ["192.168.1.15:9200", "192.168.1.11:9200"]
      index => "apachelog"
      flush_size => 2000
      idle_flush_time => 10
  }}
}

使用 filebeat 收集日志，发送到 logstash
logstash beats 配置
input{
  ... ...
  beats{
    port => 5044
  }
}

客户端配置
filebeat:
  prospectors:
    -
      paths:
        - /var/log/httpd/access_log
      input_type: log
      document_type: apachelog
  registry_file: /var/lib/filebeat/registry
output:
  logstash:
    hosts: ["192.168.1.20:5044"]
shipper:
logging:
  files:
    rotateeverybytes: 10485760 # = 10MB





1 hadoop 单机配置
/usr/local/hadoop/etc/hadoop/hadoop-env.sh
export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre"
export HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop"

分析单词出现的次数
./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar wordcount oo xx

#-----------------------------------------------------#
ALL 表示所有主机，
NODE 表示 node1,node2,node3
NN1: 表示 namenode
#-----------------------------------------------------#

完全分布式集群搭建 -- HDFS
192.168.1.10	nn01	namenode,secondarynamenode
192.168.1.11	node1	datanode
192.168.1.12	node2	datanode
192.168.1.13	node3	datanode

ALL: 配置 /etc/hosts
ALL: 安装 java-1.8.0-openjdk-devel

core-site.xml 配置
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://nn01:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/var/hadoop</value>
    </property>
</configuration>

ALL: 创建 /var/hadoop

hdfs-site.xml 配置
<configuration>
    <property>
        <name>dfs.namenode.http-address</name>
        <value>nn01:50070</value>
    </property>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>nn01:50090</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
</configuration>

ALL: 同步配置到所有主机

NN01: 格式化 namenode
./bin/hdfs namenode -format

NN01: 启动集群
./sbin/start-dfs.sh
停止集群可以使用 ./sbin/stop-dfs.sh

ALL: 验证角色 jps

NN01: 验证集群是否组建成功
./bin/hdfs dfsadmin -report

服务启动日志路径 /usr/local/hadoop/logs

mapred-site.xml 配置
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>

yarn-site.xml 配置
<configuration>

<!-- Site specific YARN configuration properties -->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>nn01</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>

ALL: 同步配置到主机
NN1: 启动服务 ./sbin/start-yarn.sh
ALL: 验证角色 jps 
NN1: 验证节点状态 ./bin/yarn node -list

增加修复节点
按照单机方法安装一台机器，部署运行的 java 环境
拷贝 namenode 的文件到本机
启动 datanode
./sbin/hadoop-daemons.sh start datanode
设置同步带宽
./bin/hdfs dfsadmin -setBalancerBandwidth 60000000
./sbin/start-balancer.sh

删除节点
<property>
    <name>dfs.hosts.exclude</name>
    <value>/usr/local/hadoop/etc/hadoop/exclude</value>
</property>

开始导出数据
./bin/hdfs dfsadmin -refreshNodes

查看状态
Normal   正常状态
Decommissioned  in  Program  数据正在迁移
Decommissioned   数据迁移完成

yarn 增加 nodemanager
./sbin/yarn-daemon.sh start nodemanager
yarn 停止 nodemanager
./sbin/yarn-daemon.sh stop  nodemanager
yarn 查看节点状态
./bin/yarn node -list

NFS 网关
1 配置 /etc/hosts (NFSGW)
192.168.1.10	nn01
192.168.1.11	node1
192.168.1.12	node2
192.168.1.13	node3
192.168.1.15	nfsgw

2 添加用户(nfsgw, nn01)
groupadd -g 200 nsd1803
useradd -u 200 -g 200 nsd1803

NN01: 3 停止集群
./sbin/stop-all.sh

NN01: 4 增加配置 core-site.xml
    <property>
        <name>hadoop.proxyuser.nsd1803.groups</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.nsd1803.hosts</name>
        <value>*</value>
    </property>
NN01: 5 同步配置到 node1 node2 node3
NN01: 6 启动集群   ./sbin/start-dfs.sh
NN01: 7 查看状态
./bin/hdfs dfsadmin -report

NFSGW: 安装 java-1.8.0-openjdk-devel
NFSGW: 同步 nn01 的 /usr/local/hadoop 到NFSGW的相同目录下
NFSGW: hdfs-site.xml 增加配置
    <property>
        <name>nfs.exports.allowed.hosts</name>
        <value>* rw</value>
    </property>
    <property>
        <name>nfs.dump.dir</name>
        <value>/var/nfstmp</value>
    </property>

NFSGW: 创建转储目录，并给用户 nsd1803 赋权
mkdir /var/nfstmp
chown nsd1803:nsd1803 /var/nfstmp

NFSGW: 给 /usr/local/hadoop/logs 赋权
setfacl -m u:nsd1803:rwx

创建数据根目录 /var/hadoop
mkdir /var/hadoop

Client: 安装 nfs-utils 
mount 共享目录
mount -t nfs -o vers=3,proto=tcp,nolock,noatime,sync,noacl 192.168.1.15:/  /mnt/

Client: 实现开机自动挂载

查看注册服务
rpcinfo -p 192.168.1.15
查看共享目录
showmount -e 192.168.1.15

zookeeper 安装
1 配置 /etc/hosts ,所有集群主机可以相互 ping 通
2 安装 java-1.8.0-openjdk-devel
3 zookeeper 解压拷贝到 /usr/local/zookeeper
4 配置文件改名，并在最后添加配置
mv zoo_sample.cfg  zoo.cfg
zoo.cfg 最后添加配置
server.1=node1:2888:3888
server.2=node2:2888:3888
server.3=node3:2888:3888
server.4=nn01:2888:3888:observer
5 拷贝 /usr/local/zookeeper 到其他集群主机
6 创建 mkdir /tmp/zookeeper
ALL: 7 创建 myid 文件，id 必须与配置文件里主机名对应的 server.(id) 一致 echo 1 >
/tmp/zookeeper/myid
8 启动服务，单启动一台无法查看状态，需要启动全部集群以后才能查看状态
/usr/local/zookeeper/bin/zkServer.sh start
9 查看状态
/usr/local/zookeeper/bin/zkServer.sh status

利用 api 查看状态的脚本
#!/bin/bash
function getstatus(){
    exec 9<>/dev/tcp/$1/2181 2>/dev/null
    echo stat >&9
    MODE=$(cat <&9 |grep -Po "(?<=Mode:).*")
    exec 9<&-
    echo ${MODE:-NULL}
}
for i in node{1..3} nn01;do
    echo -ne "${i}\t"
    getstatus ${i}
done

kafka 搭建
1 下载解压 kafka 压缩包
2 把 kafka 拷贝到 /usr/local/kafka 下面
3 修改配置文件 /usr/local/kafka/config/server.properties
broker.id=11
zookeeper.connect=node1:2181,node2:2181,node3:2181
4 拷贝 kafka 到其他主机，并修改 broker.id ,不能重复
5 启动 kafka 集群
/usr/local/kafka/bin/kafka-server-start.sh -daemon
/usr/local/kafka/config/server.properties

验证集群
创建一个 topic 
./bin/kafka-topics.sh --create --partitions 1 --replication-factor 1 --zookeeper node3:2181 --topic nsd1803
生产者
./bin/kafka-console-producer.sh --broker-list node2:9092 --topic nsd1803
消费者
./bin/kafka-console-consumer.sh --bootstrap-server node1:9092 --topic nsd1803

hadoop 高可用
ALL: 配置 /etc/hosts
192.168.1.10	nn01
192.168.1.20	nn02
192.168.1.11	node1
192.168.1.12	node2
192.168.1.13	node3
ALL: 除了 zookeeper 其他 hadoop ，kafka 服务全部停止
ALL: 初始化 hdfs 集群，删除 /var/hadoop/*
NN2: 关闭 ssh key 验证，部署公钥私钥
StrictHostKeyChecking no
scp nn01:/root/.ssh/id_rsa /root/.ssh/
scp nn01:/root/.ssh/authorized_keys /root/.ssh/

NN1: 配置 core-site.xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://nsdcluster</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/var/hadoop</value>
    </property>
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>node1:2181,node2:2181,node3:2181</value>
    </property>
    <property>
        <name>hadoop.proxyuser.nsd1803.groups</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.nsd1803.hosts</name>
        <value>*</value>
    </property>
</configuration>

配置 hdfs-site.xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.nameservices</name>
        <value>nsdcluster</value>
    </property>
    <property>
        <name>dfs.ha.namenodes.nsdcluster</name>
        <value>nn1,nn2</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.nsdcluster.nn1</name>
        <value>nn01:8020</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.nsdcluster.nn2</name>
        <value>nn02:8020</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.nsdcluster.nn1</name>
        <value>nn01:50070</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.nsdcluster.nn2</name>
        <value>nn02:50070</value>
    </property>
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://node1:8485;node2:8485;node3:8485/nsdcluster</value>
    </property>
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/var/hadoop/journal</value>
    </property>
    <property>
        <name>dfs.client.failover.proxy.provider.nsdcluster</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/root/.ssh/id_rsa</value>
    </property>
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
</configuration>

yarn-site.xml 配置文件
<configuration>

<!-- Site specific YARN configuration properties -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
    </property> 
    <property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2</value>
    </property>
    <property>
        <name>yarn.resourcemanager.recovery.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.store.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
    </property>
    <property>
        <name>yarn.resourcemanager.zk-address</name>
        <value>node1:2181,node2:2181,node3:2181</value>
    </property>
    <property>
        <name>yarn.resourcemanager.cluster-id</name>
        <value>yarn-ha</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>nn01</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>nn02</value>
    </property>
</configuration>

#-----------------------------------------------------#
初始化启动集群
ALL: 所有机器
nodeX： node1    node2    node3
NN1: nn01
NN2: nn02
#-----------------------------------------------------#
ALL:  同步配置到所有集群机器

NN1: 初始化ZK集群  ./bin/hdfs zkfc -formatZK

nodeX:  启动 journalnode 服务 
        ./sbin/hadoop-daemon.sh start journalnode

NN1: 格式化  ./bin/hdfs  namenode  -format

NN2: 数据同步到本地 /var/hadoop/dfs

NN1: 初始化 JNS
        ./bin/hdfs namenode -initializeSharedEdits

nodeX: 停止 journalnode 服务
        ./sbin/hadoop-daemon.sh stop journalnode

#-----------------------------------------------------#
启动集群
NN1: ./sbin/start-all.sh
NN2: ./sbin/yarn-daemon.sh start resourcemanager

查看集群状态
./bin/hdfs haadmin -getServiceState nn1  
./bin/hdfs haadmin -getServiceState nn2
./bin/yarn rmadmin -getServiceState rm1
./bin/yarn rmadmin -getServiceState rm2

./bin/hdfs dfsadmin -report
./bin/yarn  node  -list

访问集群：
./bin/hadoop  fs -ls  /
./bin/hadoop  fs -mkdir hdfs://nsdcluster/input

验证高可用，关闭 active namenode
./sbin/hadoop-daemon.sh stop namenode
./sbin/yarn-daemon.sh stop resourcemanager

恢复节点
./sbin/hadoop-daemon.sh stop namenode
./sbin/yarn-daemon.sh stop resourcemanager

